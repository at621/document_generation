{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920d5c76-68b5-49b3-b4e1-eaacf89f5870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea6eb66b-7dae-47f3-a108-b2a59057090f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 13:05:38,221 - __main__ - INFO - ================================================================================\n",
      "2025-06-25 13:05:38,223 - __main__ - INFO - Initializing embeddings model: text-embedding-3-large\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] langchain_docling is available\n",
      "\n",
      "=== Testing Imports ===\n",
      "[OK] docling is installed\n",
      "[OK] langchain_docling is installed\n",
      "  [OK] DoclingLoader imported successfully\n",
      "  [OK] ExportType imported from langchain_docling.loader\n",
      "    Available export types: ['MARKDOWN', 'DOC_CHUNKS']\n",
      "[OK] langchain_openai is installed\n",
      "[OK] langchain_text_splitters is installed\n",
      "======================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 13:05:40,217 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 13:05:40,303 - __main__ - INFO - ✓ Embeddings model initialized successfully (dim: 3072)\n",
      "2025-06-25 13:05:40,305 - __main__ - INFO - Configuring Docling pipeline...\n",
      "2025-06-25 13:05:40,308 - __main__ - INFO -   ✓ Table extraction enabled\n",
      "2025-06-25 13:05:40,309 - __main__ - INFO -   ✓ Figure description enabled (VLM: smolvlm)\n",
      "2025-06-25 13:05:40,316 - __main__ - INFO - ✓ Docling pipeline configured\n",
      "2025-06-25 13:05:40,318 - __main__ - INFO - Setting up chunking strategy...\n",
      "2025-06-25 13:05:40,538 - __main__ - INFO -   ✓ Using Docling HybridChunker\n",
      "2025-06-25 13:05:40,539 - __main__ - INFO - Computing category embeddings...\n",
      "2025-06-25 13:05:40,750 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 13:05:40,836 - __main__ - INFO -   ✓ Computed embedding for 'Recognition'\n",
      "2025-06-25 13:05:41,044 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 13:05:41,050 - __main__ - INFO -   ✓ Computed embedding for 'Measurement'\n",
      "2025-06-25 13:05:41,329 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 13:05:41,418 - __main__ - INFO -   ✓ Computed embedding for 'Disclosure'\n",
      "2025-06-25 13:05:41,632 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 13:05:41,638 - __main__ - INFO -   ✓ Computed embedding for 'Presentation'\n",
      "2025-06-25 13:05:41,831 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 13:05:41,919 - __main__ - INFO -   ✓ Computed embedding for 'Transition'\n",
      "2025-06-25 13:05:41,921 - __main__ - INFO - ✓ Computed embeddings for 5 categories\n",
      "2025-06-25 13:05:41,923 - root - INFO - Found 2 PDF files to process\n",
      "2025-06-25 13:05:41,925 - __main__ - INFO - \n",
      "Starting processing of 2 PDF documents\n",
      "2025-06-25 13:05:41,926 - __main__ - INFO - ================================================================================\n",
      "Processing PDFs:   0%|                                                                          | 0/2 [00:00<?, ?pdf/s]2025-06-25 13:05:41,932 - __main__ - INFO - \n",
      "[1/2] Processing: IFRS9 monitoring report.pdf\n",
      "2025-06-25 13:05:41,933 - __main__ - INFO -   -> Converting with Docling...\n",
      "2025-06-25 13:05:42,014 - docling.document_converter - INFO - Going to convert document batch...\n",
      "2025-06-25 13:05:42,016 - docling.document_converter - INFO - Initializing pipeline for StandardPdfPipeline with options hash bf7bb4cb6ca03fd78da42a7bf138bee6\n",
      "2025-06-25 13:05:42,055 - docling.models.factories.base_factory - INFO - Loading plugin 'docling_defaults'\n",
      "2025-06-25 13:05:42,056 - docling.models.factories - INFO - Registered ocr engines: ['easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-06-25 13:05:42,167 - docling.utils.accelerator_utils - INFO - Accelerator device: 'cuda:0'\n",
      "2025-06-25 13:05:44,531 - docling.utils.accelerator_utils - INFO - Accelerator device: 'cuda:0'\n",
      "2025-06-25 13:05:45,606 - docling.utils.accelerator_utils - INFO - Accelerator device: 'cuda:0'\n",
      "2025-06-25 13:05:46,267 - docling.models.factories.base_factory - INFO - Loading plugin 'docling_defaults'\n",
      "2025-06-25 13:05:46,270 - docling.models.factories - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-06-25 13:05:46,406 - docling.utils.accelerator_utils - INFO - Accelerator device: 'cuda:0'\n",
      "2025-06-25 13:05:47,675 - docling.utils.accelerator_utils - INFO - Accelerator device: 'cuda:0'\n",
      "2025-06-25 13:05:48,070 - docling.pipeline.base_pipeline - INFO - Processing document IFRS9 monitoring report.pdf\n",
      "2025-06-25 13:25:36,534 - docling.document_converter - INFO - Finished converting document IFRS9 monitoring report.pdf in 1194.60 sec.\n",
      "2025-06-25 13:25:36,536 - __main__ - INFO -   -> Document has 120 pages\n",
      "2025-06-25 13:25:36,538 - __main__ - INFO -   -> Using MARKDOWN export with hierarchical splitting\n",
      "2025-06-25 13:25:36,578 - docling.document_converter - INFO - Going to convert document batch...\n",
      "2025-06-25 13:25:36,581 - docling.pipeline.base_pipeline - INFO - Processing document IFRS9 monitoring report.pdf\n",
      "2025-06-25 13:45:43,846 - docling.document_converter - INFO - Finished converting document IFRS9 monitoring report.pdf in 1207.31 sec.\n",
      "2025-06-25 13:45:44,215 - __main__ - INFO -   -> Loaded 1 documents from DoclingLoader\n",
      "2025-06-25 13:45:44,231 - __main__ - INFO -   -> Created 147 hierarchical chunks from 1 documents\n",
      "2025-06-25 13:45:44,232 - __main__ - INFO -   -> Created 147 chunks\n",
      "2025-06-25 13:45:44,233 - __main__ - INFO -   -> Creating embeddings for 147 chunks...\n",
      "2025-06-25 13:45:45,068 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 13:45:45,987 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 13:45:46,544 - __main__ - INFO -     Processed 100/147 chunks\n",
      "2025-06-25 13:45:47,761 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 13:45:48,242 - __main__ - INFO -   -> Document processing completed in 2406.31s\n",
      "2025-06-25 13:45:48,390 - __main__ - INFO -   ✓ Successfully processed in 2406.46s\n",
      "2025-06-25 13:45:48,391 - __main__ - INFO -   ✓ Generated 147 chunks\n",
      "Processing PDFs:  50%|███████████████▌               | 1/2 [40:06<40:06, 2406.46s/pdf, Success=1, Failed=0, Chunks=147]2025-06-25 13:45:48,395 - __main__ - INFO - \n",
      "[2/2] Processing: ssm.IFRS9novelrisks_202407~5e0eb30b5c.en.pdf\n",
      "2025-06-25 13:45:48,395 - __main__ - INFO -   -> Converting with Docling...\n",
      "2025-06-25 13:45:48,413 - docling.document_converter - INFO - Going to convert document batch...\n",
      "2025-06-25 13:45:48,415 - docling.pipeline.base_pipeline - INFO - Processing document ssm.IFRS9novelrisks_202407~5e0eb30b5c.en.pdf\n",
      "2025-06-25 13:48:25,986 - docling.document_converter - INFO - Finished converting document ssm.IFRS9novelrisks_202407~5e0eb30b5c.en.pdf in 157.59 sec.\n",
      "2025-06-25 13:48:25,988 - __main__ - INFO -   -> Document has 22 pages\n",
      "2025-06-25 13:48:25,989 - __main__ - INFO -   -> Using MARKDOWN export with hierarchical splitting\n",
      "2025-06-25 13:48:26,001 - docling.document_converter - INFO - Going to convert document batch...\n",
      "2025-06-25 13:48:26,004 - docling.pipeline.base_pipeline - INFO - Processing document ssm.IFRS9novelrisks_202407~5e0eb30b5c.en.pdf\n",
      "2025-06-25 13:51:06,800 - docling.document_converter - INFO - Finished converting document ssm.IFRS9novelrisks_202407~5e0eb30b5c.en.pdf in 160.81 sec.\n",
      "2025-06-25 13:51:06,848 - __main__ - INFO -   -> Loaded 1 documents from DoclingLoader\n",
      "2025-06-25 13:51:06,855 - __main__ - INFO -   -> Created 22 hierarchical chunks from 1 documents\n",
      "2025-06-25 13:51:06,857 - __main__ - INFO -   -> Created 22 chunks\n",
      "2025-06-25 13:51:06,858 - __main__ - INFO -   -> Creating embeddings for 22 chunks...\n",
      "2025-06-25 13:51:07,626 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 13:51:08,003 - __main__ - INFO -   -> Document processing completed in 319.61s\n",
      "2025-06-25 13:51:08,024 - __main__ - INFO -   ✓ Successfully processed in 319.63s\n",
      "2025-06-25 13:51:08,025 - __main__ - INFO -   ✓ Generated 22 chunks\n",
      "Processing PDFs: 100%|███████████████████████████████| 2/2 [45:26<00:00, 1363.05s/pdf, Success=2, Failed=0, Chunks=169]\n",
      "2025-06-25 13:51:08,036 - __main__ - INFO - \n",
      "================================================================================\n",
      "2025-06-25 13:51:08,038 - __main__ - INFO - PROCESSING COMPLETE - FINAL STATISTICS\n",
      "2025-06-25 13:51:08,038 - __main__ - INFO - ================================================================================\n",
      "2025-06-25 13:51:08,039 - __main__ - INFO - \n",
      "Document Processing:\n",
      "2025-06-25 13:51:08,039 - __main__ - INFO -   Total PDFs: 2\n",
      "2025-06-25 13:51:08,040 - __main__ - INFO -   Successfully processed: 2\n",
      "2025-06-25 13:51:08,041 - __main__ - INFO -   Failed: 0\n",
      "2025-06-25 13:51:08,042 - __main__ - INFO -   Success rate: 100.0%\n",
      "2025-06-25 13:51:08,042 - __main__ - INFO - \n",
      "Content Statistics:\n",
      "2025-06-25 13:51:08,043 - __main__ - INFO -   Total chunks created: 169\n",
      "2025-06-25 13:51:08,043 - __main__ - INFO -   Total pages processed: 142\n",
      "2025-06-25 13:51:08,044 - __main__ - INFO -   Average chunks per document: 84.5\n",
      "2025-06-25 13:51:08,045 - __main__ - INFO -   Tables found: 20\n",
      "2025-06-25 13:51:08,046 - __main__ - INFO -   Figures found: 62\n",
      "2025-06-25 13:51:08,047 - __main__ - INFO - \n",
      "Performance:\n",
      "2025-06-25 13:51:08,048 - __main__ - INFO -   Total processing time: 2729.86s\n",
      "2025-06-25 13:51:08,049 - __main__ - INFO -   Average time per document: 1364.93s\n",
      "2025-06-25 13:51:08,050 - __main__ - INFO - \n",
      "Categorization Results:\n",
      "2025-06-25 13:51:08,051 - __main__ - INFO -   Category 1 distribution:\n",
      "2025-06-25 13:51:08,060 - __main__ - INFO -     Transition: 54 (32.0%) - avg similarity: 0.327\n",
      "2025-06-25 13:51:08,062 - __main__ - INFO -     Disclosure: 53 (31.4%) - avg similarity: 0.278\n",
      "2025-06-25 13:51:08,064 - __main__ - INFO -     Measurement: 49 (29.0%) - avg similarity: 0.321\n",
      "2025-06-25 13:51:08,065 - __main__ - INFO -     Recognition: 8 (4.7%) - avg similarity: 0.445\n",
      "2025-06-25 13:51:08,066 - __main__ - INFO -     Presentation: 5 (3.0%) - avg similarity: 0.303\n",
      "2025-06-25 13:51:08,067 - __main__ - INFO - \n",
      "  Category 2 distribution (top 10):\n",
      "2025-06-25 13:51:08,068 - __main__ - INFO -     IFRS 9: 167 (98.8%)\n",
      "2025-06-25 13:51:08,069 - __main__ - INFO -     General: 2 (1.2%)\n",
      "2025-06-25 13:51:08,070 - __main__ - INFO - \n",
      "Content Analysis:\n",
      "2025-06-25 13:51:08,071 - __main__ - INFO -   Average chunk length: 2222 characters\n",
      "2025-06-25 13:51:08,071 - __main__ - INFO -   Chunks with tables: 20 (11.8%)\n",
      "2025-06-25 13:51:08,072 - __main__ - INFO -   Chunks with figures: 62 (36.7%)\n",
      "2025-06-25 13:51:08,073 - __main__ - INFO - \n",
      "Sample Document Structure:\n",
      "2025-06-25 13:51:08,074 - __main__ - INFO - \n",
      "================================================================================\n",
      "2025-06-25 13:51:08,886 - root - INFO - \n",
      "[OK] Saved processed data to output\\ifrs_embeddings.parquet\n",
      "2025-06-25 13:51:08,912 - root - INFO - [OK] Saved statistics and sample files\n",
      "2025-06-25 13:51:08,913 - root - INFO - \n",
      "[COMPLETE] Processing pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "IFRS Document Processing Pipeline\n",
    "=================================\n",
    "\n",
    "This pipeline processes IFRS PDF documents using Docling for parsing and LangChain for processing,\n",
    "creating embeddings with OpenAI, and storing the results in a Pandas DataFrame saved as Parquet.\n",
    "\n",
    "Pipeline Flow:\n",
    "PDFs → Docling (with VLM enrichment) → Smart Chunking → Dual Categorization → \n",
    "OpenAI Embeddings → Pandas DataFrame → Parquet Storage\n",
    "\n",
    "Chunking Strategy:\n",
    "-----------------\n",
    "The pipeline uses MarkdownHeaderTextSplitter for hierarchical chunking, which is ideal for IFRS documents:\n",
    "- Preserves document hierarchy (Standard > Section > Subsection > Paragraph)\n",
    "- Splits on semantic boundaries (headers) rather than arbitrary token counts\n",
    "- Maintains section context in metadata for better categorization\n",
    "- Respects the natural structure of financial standards\n",
    "\n",
    "Installation Requirements:\n",
    "-------------------------\n",
    "pip install docling\n",
    "pip install langchain-openai\n",
    "pip install langchain-text-splitters\n",
    "pip install langchain-docling\n",
    "pip install pandas\n",
    "pip install numpy\n",
    "pip install tqdm\n",
    "\n",
    "Important Import Notes:\n",
    "----------------------\n",
    "Based on LangChain documentation, the correct imports are:\n",
    "    from langchain_docling import DoclingLoader\n",
    "    from langchain_docling.loader import ExportType\n",
    "\n",
    "Key Features:\n",
    "- Advanced PDF parsing with table and figure extraction\n",
    "- Vision Language Model (VLM) support for chart descriptions\n",
    "- Hierarchical-aware chunking that respects IFRS document structure\n",
    "- Dual categorization system using embedding similarity\n",
    "- Rich metadata preservation\n",
    "- Comprehensive logging and progress tracking\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from enum import Enum\n",
    "\n",
    "# Docling imports\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.chunking import HybridChunker\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Correct langchain_docling imports based on documentation\n",
    "# IMPORTANT: ExportType must be imported from langchain_docling.loader, not langchain_docling\n",
    "try:\n",
    "    from langchain_docling import DoclingLoader\n",
    "    from langchain_docling.loader import ExportType\n",
    "    LANGCHAIN_DOCLING_AVAILABLE = True\n",
    "    print(\"[OK] langchain_docling is available\")\n",
    "except ImportError as e:\n",
    "    LANGCHAIN_DOCLING_AVAILABLE = False\n",
    "    print(f\"Warning: langchain_docling not available: {e}\")\n",
    "    print(\"Using direct Docling integration instead.\")\n",
    "    print(\"\\nTo fix, install with: pip install langchain-docling\")\n",
    "    # Create a dummy ExportType for compatibility\n",
    "    class ExportType(Enum):\n",
    "        MARKDOWN = \"markdown\"\n",
    "        DOC_CHUNKS = \"doc_chunks\"\n",
    "        HTML = \"html\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ChunkingConfig:\n",
    "    \"\"\"Configuration for document chunking strategy\"\"\"\n",
    "    min_chunk_tokens: int = 300\n",
    "    max_chunk_tokens: int = 500\n",
    "    overlap_tokens: int = 50\n",
    "    respect_sections: bool = True  # Use hierarchical header-based splitting\n",
    "    keep_tables_intact: bool = True\n",
    "    include_figure_descriptions: bool = True\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ProcessingConfig:\n",
    "    \"\"\"Main configuration for the processing pipeline\"\"\"\n",
    "    # Docling settings\n",
    "    enable_table_extraction: bool = True\n",
    "    enable_figure_descriptions: bool = True\n",
    "    figure_description_model: str = \"smolvlm\"  # or \"granite\" for better quality\n",
    "    images_scale: int = 2  # Quality of extracted images\n",
    "    \n",
    "    # Embedding settings\n",
    "    embedding_model: str = \"text-embedding-3-large\"\n",
    "    embedding_dimensions: int = 3072\n",
    "    batch_size: int = 50\n",
    "    \n",
    "    # Chunking settings\n",
    "    chunking: ChunkingConfig = None\n",
    "    \n",
    "    # Logging settings\n",
    "    log_level: str = \"INFO\"\n",
    "    log_file: Optional[str] = \"ifrs_processing.log\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.chunking is None:\n",
    "            self.chunking = ChunkingConfig()\n",
    "\n",
    "\n",
    "class ProcessingStats:\n",
    "    \"\"\"Track processing statistics\"\"\"\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "        self.total_pdfs = 0\n",
    "        self.processed_pdfs = 0\n",
    "        self.failed_pdfs = 0\n",
    "        self.total_chunks = 0\n",
    "        self.total_pages = 0\n",
    "        self.tables_found = 0\n",
    "        self.figures_found = 0\n",
    "        self.processing_times = {}\n",
    "    \n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get summary statistics\"\"\"\n",
    "        elapsed_time = time.time() - self.start_time\n",
    "        return {\n",
    "            'total_pdfs': self.total_pdfs,\n",
    "            'processed_pdfs': self.processed_pdfs,\n",
    "            'failed_pdfs': self.failed_pdfs,\n",
    "            'success_rate': (self.processed_pdfs / self.total_pdfs * 100) if self.total_pdfs > 0 else 0,\n",
    "            'total_chunks': self.total_chunks,\n",
    "            'total_pages': self.total_pages,\n",
    "            'avg_chunks_per_doc': self.total_chunks / self.processed_pdfs if self.processed_pdfs > 0 else 0,\n",
    "            'tables_found': self.tables_found,\n",
    "            'figures_found': self.figures_found,\n",
    "            'total_time_seconds': elapsed_time,\n",
    "            'avg_time_per_doc': elapsed_time / self.processed_pdfs if self.processed_pdfs > 0 else 0\n",
    "        }\n",
    "\n",
    "\n",
    "class IFRSDocumentProcessor:\n",
    "    \"\"\"\n",
    "    Main processor for IFRS documents with intelligent chunking and categorization.\n",
    "    \n",
    "    Uses embedding-based categorization for semantic understanding of content.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        category_list_1: List[str],\n",
    "        category_list_2: List[str],\n",
    "        config: ProcessingConfig = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the processor with categorization lists and configuration.\n",
    "        \n",
    "        Args:\n",
    "            category_list_1: First categorization scheme (e.g., ['Recognition', 'Measurement', 'Disclosure'])\n",
    "            category_list_2: Second categorization scheme (e.g., ['IFRS 15', 'IFRS 16', 'IAS 12'])\n",
    "            config: Processing configuration\n",
    "        \"\"\"\n",
    "        self.categories_1 = category_list_1\n",
    "        self.categories_2 = category_list_2\n",
    "        self.config = config or ProcessingConfig()\n",
    "        self.stats = ProcessingStats()\n",
    "        \n",
    "        # Setup logging\n",
    "        self._setup_logging()\n",
    "        \n",
    "        # Initialize components\n",
    "        self._setup_embeddings()\n",
    "        self._setup_docling()\n",
    "        self._setup_chunking()\n",
    "        \n",
    "        # Category descriptions for embedding-based classification\n",
    "        self.category_descriptions = {\n",
    "            'Recognition': \"\"\"\n",
    "                This text discusses when and how to recognize items in financial statements,\n",
    "                including initial recognition criteria, derecognition requirements, and the\n",
    "                timing of when transactions should be recorded in the accounts.\n",
    "            \"\"\",\n",
    "            'Measurement': \"\"\"\n",
    "                This text covers how to measure financial items, including fair value measurement,\n",
    "                cost basis, amortized cost, present value calculations, carrying amounts,\n",
    "                revaluation methods, and subsequent measurement requirements.\n",
    "            \"\"\",\n",
    "            'Disclosure': \"\"\"\n",
    "                This text relates to disclosure requirements, notes to financial statements,\n",
    "                required disclosures, presentation of information, and what information\n",
    "                entities must provide to users of financial statements.\n",
    "            \"\"\",\n",
    "            'Presentation': \"\"\"\n",
    "                This text addresses how items should be presented in financial statements,\n",
    "                including line items, classification of items, statement formats, and\n",
    "                requirements for separate presentation of different elements.\n",
    "            \"\"\",\n",
    "            'Transition': \"\"\"\n",
    "                This text covers transition provisions, effective dates, first-time adoption\n",
    "                requirements, retrospective or prospective application, and grandfathering\n",
    "                provisions for new or amended standards.\n",
    "            \"\"\"\n",
    "        }\n",
    "        \n",
    "        # Pre-compute category embeddings\n",
    "        self.category_embeddings = self._compute_category_embeddings()\n",
    "    \n",
    "    def _setup_logging(self):\n",
    "        \"\"\"Configure comprehensive logging with UTF-8 encoding support\"\"\"\n",
    "        log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        \n",
    "        # Create handlers with UTF-8 encoding\n",
    "        handlers = []\n",
    "        \n",
    "        # Console handler with UTF-8 encoding\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setFormatter(logging.Formatter(log_format))\n",
    "        handlers.append(console_handler)\n",
    "        \n",
    "        # File handler with UTF-8 encoding\n",
    "        if self.config.log_file:\n",
    "            file_handler = logging.FileHandler(self.config.log_file, encoding='utf-8')\n",
    "            file_handler.setFormatter(logging.Formatter(log_format))\n",
    "            handlers.append(file_handler)\n",
    "        \n",
    "        # Configure root logger\n",
    "        logging.basicConfig(\n",
    "            level=getattr(logging, self.config.log_level),\n",
    "            format=log_format,\n",
    "            handlers=handlers,\n",
    "            encoding='utf-8',  # Set default encoding\n",
    "            force=True  # Force reconfiguration\n",
    "        )\n",
    "        \n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # For Windows compatibility, detect if Unicode is supported\n",
    "        try:\n",
    "            import sys\n",
    "            if sys.platform == 'win32':\n",
    "                # Try to enable Unicode in Windows console\n",
    "                import os\n",
    "                os.system('chcp 65001 >nul 2>&1')  # Set console to UTF-8\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        self.logger.info(\"=\"*80)\n",
    "    \n",
    "    def _get_check_mark(self) -> str:\n",
    "        \"\"\"Get appropriate check mark based on system encoding support\"\"\"\n",
    "        try:\n",
    "            # Test if we can encode Unicode\n",
    "            '✓'.encode(sys.stdout.encoding or 'utf-8')\n",
    "            return '✓'\n",
    "        except (UnicodeEncodeError, AttributeError):\n",
    "            # Fallback to ASCII\n",
    "            return '[OK]'\n",
    "    \n",
    "    def _get_cross_mark(self) -> str:\n",
    "        \"\"\"Get appropriate cross mark based on system encoding support\"\"\"\n",
    "        try:\n",
    "            '✗'.encode(sys.stdout.encoding or 'utf-8')\n",
    "            return '✗'\n",
    "        except (UnicodeEncodeError, AttributeError):\n",
    "            # Fallback to ASCII\n",
    "            return '[FAIL]'\n",
    "        self.logger.info(\"IFRS Document Processing Pipeline Started\")\n",
    "        self.logger.info(f\"Timestamp: {datetime.now().isoformat()}\")\n",
    "        self.logger.info(f\"Configuration: {json.dumps(self.config.__dict__, indent=2, default=str)}\")\n",
    "        self.logger.info(\"=\"*80)\n",
    "    \n",
    "    def _setup_embeddings(self):\n",
    "        \"\"\"Initialize OpenAI embeddings model\"\"\"\n",
    "        self.logger.info(f\"Initializing embeddings model: {self.config.embedding_model}\")\n",
    "        try:\n",
    "            self.embeddings_model = OpenAIEmbeddings(\n",
    "                model=self.config.embedding_model,\n",
    "                dimensions=self.config.embedding_dimensions\n",
    "            )\n",
    "            # Test embedding\n",
    "            test_embedding = self.embeddings_model.embed_query(\"test\")\n",
    "            self.logger.info(f\"{self._get_check_mark()} Embeddings model initialized successfully (dim: {len(test_embedding)})\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to initialize embeddings model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _setup_docling(self):\n",
    "        \"\"\"Configure Docling with enrichment options\"\"\"\n",
    "        self.logger.info(\"Configuring Docling pipeline...\")\n",
    "        self.pipeline_options = PdfPipelineOptions()\n",
    "        \n",
    "        # Enable table extraction\n",
    "        if self.config.enable_table_extraction:\n",
    "            self.pipeline_options.do_table_structure = True\n",
    "            self.logger.info(f\"  {self._get_check_mark()} Table extraction enabled\")\n",
    "            \n",
    "        # Enable figure description with VLM\n",
    "        if self.config.enable_figure_descriptions:\n",
    "            self.pipeline_options.generate_picture_images = True\n",
    "            self.pipeline_options.images_scale = self.config.images_scale\n",
    "            self.pipeline_options.do_picture_classification = True\n",
    "            self.pipeline_options.do_picture_description = True\n",
    "            self.logger.info(f\"  {self._get_check_mark()} Figure description enabled (VLM: {self.config.figure_description_model})\")\n",
    "        \n",
    "        # Create converter\n",
    "        self.converter = DocumentConverter(\n",
    "            format_options={\n",
    "                InputFormat.PDF: PdfFormatOption(pipeline_options=self.pipeline_options)\n",
    "            }\n",
    "        )\n",
    "        self.logger.info(f\"{self._get_check_mark()} Docling pipeline configured\")\n",
    "    \n",
    "    def _setup_chunking(self):\n",
    "        \"\"\"Initialize chunking strategy\"\"\"\n",
    "        self.logger.info(\"Setting up chunking strategy...\")\n",
    "        \n",
    "        if LANGCHAIN_DOCLING_AVAILABLE:\n",
    "            self.chunker = HybridChunker(\n",
    "                tokenizer=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                max_tokens=self.config.chunking.max_chunk_tokens,\n",
    "                min_overlap_tokens=self.config.chunking.overlap_tokens,\n",
    "                split_on_sentences=True\n",
    "            )\n",
    "            self.logger.info(f\"  {self._get_check_mark()} Using Docling HybridChunker\")\n",
    "        else:\n",
    "            # Fallback to LangChain splitter\n",
    "            self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=self.config.chunking.max_chunk_tokens * 4,  # Approximate chars\n",
    "                chunk_overlap=self.config.chunking.overlap_tokens * 4,\n",
    "                separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "            )\n",
    "            self.logger.info(f\"  {self._get_check_mark()} Using LangChain RecursiveCharacterTextSplitter\")\n",
    "    \n",
    "    def _compute_category_embeddings(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Pre-compute embeddings for each category based on their descriptions\"\"\"\n",
    "        self.logger.info(\"Computing category embeddings...\")\n",
    "        category_embeddings = {}\n",
    "        \n",
    "        for category, description in self.category_descriptions.items():\n",
    "            embedding = self.embeddings_model.embed_query(description.strip())\n",
    "            category_embeddings[category] = np.array(embedding)\n",
    "            self.logger.info(f\"  {self._get_check_mark()} Computed embedding for '{category}'\")\n",
    "            \n",
    "        self.logger.info(f\"{self._get_check_mark()} Computed embeddings for {len(category_embeddings)} categories\")\n",
    "        return category_embeddings\n",
    "    \n",
    "    def _compute_cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "        \"\"\"Compute cosine similarity between two vectors\"\"\"\n",
    "        dot_product = np.dot(vec1, vec2)\n",
    "        norm_product = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
    "        return dot_product / norm_product if norm_product > 0 else 0.0\n",
    "    \n",
    "    def process_documents(self, pdf_paths: List[Path]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Main processing pipeline for IFRS documents.\n",
    "        \n",
    "        Args:\n",
    "            pdf_paths: List of paths to PDF files\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with processed chunks, embeddings, and categorizations\n",
    "        \"\"\"\n",
    "        self.stats.total_pdfs = len(pdf_paths)\n",
    "        self.logger.info(f\"\\nStarting processing of {len(pdf_paths)} PDF documents\")\n",
    "        self.logger.info(\"=\"*80)\n",
    "        \n",
    "        all_data = []\n",
    "        \n",
    "        with tqdm(total=len(pdf_paths), desc=\"Processing PDFs\", unit=\"pdf\") as pbar:\n",
    "            for i, pdf_path in enumerate(pdf_paths, 1):\n",
    "                self.logger.info(f\"\\n[{i}/{len(pdf_paths)}] Processing: {pdf_path.name}\")\n",
    "                doc_start_time = time.time()\n",
    "                \n",
    "                try:\n",
    "                    # Process single document\n",
    "                    chunks_data = self._process_single_document(pdf_path)\n",
    "                    all_data.extend(chunks_data)\n",
    "                    \n",
    "                    # Update stats\n",
    "                    self.stats.processed_pdfs += 1\n",
    "                    doc_time = time.time() - doc_start_time\n",
    "                    self.stats.processing_times[pdf_path.name] = doc_time\n",
    "                    \n",
    "                    self.logger.info(f\"  {self._get_check_mark()} Successfully processed in {doc_time:.2f}s\")\n",
    "                    self.logger.info(f\"  {self._get_check_mark()} Generated {len(chunks_data)} chunks\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.stats.failed_pdfs += 1\n",
    "                    self.logger.error(f\"  {self._get_cross_mark()} Error processing {pdf_path.name}: {str(e)}\")\n",
    "                    self.logger.exception(\"Full traceback:\")\n",
    "                \n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\n",
    "                    'Success': self.stats.processed_pdfs,\n",
    "                    'Failed': self.stats.failed_pdfs,\n",
    "                    'Chunks': self.stats.total_chunks\n",
    "                })\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(all_data)\n",
    "        \n",
    "        # Log final statistics\n",
    "        self._log_final_statistics(df)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _process_single_document(self, pdf_path: Path) -> List[Dict]:\n",
    "        \"\"\"Process a single PDF document\"\"\"\n",
    "        doc_start_time = time.time()\n",
    "        \n",
    "        # Convert document with Docling\n",
    "        self.logger.info(f\"  -> Converting with Docling...\")\n",
    "        conversion_result = self.converter.convert(str(pdf_path))\n",
    "        doc = conversion_result.document\n",
    "        \n",
    "        # Log document info\n",
    "        num_pages = len(doc.pages) if hasattr(doc, 'pages') else 0\n",
    "        self.stats.total_pages += num_pages\n",
    "        self.logger.info(f\"  -> Document has {num_pages} pages\")\n",
    "        \n",
    "        # Create chunks\n",
    "        chunks = self._create_chunks(doc, pdf_path)\n",
    "        self.logger.info(f\"  -> Created {len(chunks)} chunks\")\n",
    "        \n",
    "        # Process chunks with embeddings\n",
    "        chunks_data = self._process_chunks_with_embeddings(chunks, pdf_path)\n",
    "        \n",
    "        doc_time = time.time() - doc_start_time\n",
    "        self.logger.info(f\"  -> Document processing completed in {doc_time:.2f}s\")\n",
    "        \n",
    "        return chunks_data\n",
    "    \n",
    "    def _create_chunks(self, doc, pdf_path: Path) -> List[Document]:\n",
    "        \"\"\"Create chunks from Docling document following LangChain documentation pattern\"\"\"\n",
    "        if LANGCHAIN_DOCLING_AVAILABLE:\n",
    "            # Determine export type based on configuration\n",
    "            if self.config.chunking.respect_sections:\n",
    "                # Use MARKDOWN export for hierarchical splitting\n",
    "                export_type = ExportType.MARKDOWN\n",
    "                self.logger.info(\"  -> Using MARKDOWN export with hierarchical splitting\")\n",
    "            else:\n",
    "                # Use DOC_CHUNKS for standard chunking\n",
    "                export_type = ExportType.DOC_CHUNKS\n",
    "                self.logger.info(\"  -> Using DOC_CHUNKS export\")\n",
    "            \n",
    "            # Load document with DoclingLoader\n",
    "            loader = DoclingLoader(\n",
    "                file_path=str(pdf_path),\n",
    "                export_type=export_type,\n",
    "                converter=self.converter\n",
    "            )\n",
    "            docs = loader.load()\n",
    "            self.logger.info(f\"  -> Loaded {len(docs)} documents from DoclingLoader\")\n",
    "            \n",
    "            # Process based on export type (following LangChain documentation pattern)\n",
    "            if export_type == ExportType.DOC_CHUNKS:\n",
    "                # With DOC_CHUNKS, documents are already chunked\n",
    "                splits = docs\n",
    "                self.logger.info(f\"  -> Using {len(splits)} pre-chunked documents\")\n",
    "                \n",
    "            elif export_type == ExportType.MARKDOWN:\n",
    "                # With MARKDOWN, we need to split using MarkdownHeaderTextSplitter\n",
    "                from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "                \n",
    "                # Define headers relevant to IFRS documents\n",
    "                splitter = MarkdownHeaderTextSplitter(\n",
    "                    headers_to_split_on=[\n",
    "                        (\"#\", \"Standard\"),      # IFRS 15\n",
    "                        (\"##\", \"Section\"),      # Objective, Scope  \n",
    "                        (\"###\", \"Subsection\"),  # Core principle\n",
    "                        (\"####\", \"Paragraph\"),  # Detailed requirements\n",
    "                    ],\n",
    "                    strip_headers=False  # Keep headers in content for context\n",
    "                )\n",
    "                \n",
    "                # Split all documents (following documentation pattern)\n",
    "                splits = [\n",
    "                    split for doc in docs \n",
    "                    for split in splitter.split_text(doc.page_content)\n",
    "                ]\n",
    "                self.logger.info(f\"  -> Created {len(splits)} hierarchical chunks from {len(docs)} documents\")\n",
    "                \n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected export type: {export_type}\")\n",
    "            \n",
    "            # Enhance metadata for all splits\n",
    "            for i, split in enumerate(splits):\n",
    "                split.metadata.update({\n",
    "                    'source': str(pdf_path.name),\n",
    "                    'chunk_index': i,\n",
    "                    'export_type': export_type.value if hasattr(export_type, 'value') else str(export_type)\n",
    "                })\n",
    "            \n",
    "            return splits\n",
    "            \n",
    "        else:\n",
    "            # Fallback: convert to markdown and chunk with hierarchy\n",
    "            self.logger.info(\"  -> Using direct Docling conversion (fallback mode)\")\n",
    "            markdown_content = doc.export_to_markdown()\n",
    "            \n",
    "            if self.config.chunking.respect_sections:\n",
    "                # Use MarkdownHeaderTextSplitter for hierarchical chunking\n",
    "                from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "                \n",
    "                splitter = MarkdownHeaderTextSplitter(\n",
    "                    headers_to_split_on=[\n",
    "                        (\"#\", \"Standard\"),\n",
    "                        (\"##\", \"Section\"),\n",
    "                        (\"###\", \"Subsection\"),\n",
    "                        (\"####\", \"Paragraph\"),\n",
    "                    ],\n",
    "                    strip_headers=False\n",
    "                )\n",
    "                \n",
    "                # Create base document\n",
    "                base_doc = Document(\n",
    "                    page_content=markdown_content,\n",
    "                    metadata={\n",
    "                        'source': str(pdf_path.name),\n",
    "                        'num_pages': len(doc.pages) if hasattr(doc, 'pages') else 0\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                # Split and create chunks\n",
    "                splits = splitter.split_text(base_doc.page_content)\n",
    "                \n",
    "                # Convert to Document objects with metadata\n",
    "                chunks = []\n",
    "                for i, split in enumerate(splits):\n",
    "                    # Inherit base metadata and add chunk-specific info\n",
    "                    metadata = base_doc.metadata.copy()\n",
    "                    metadata.update(split.metadata if hasattr(split, 'metadata') else {})\n",
    "                    metadata['chunk_index'] = i\n",
    "                    metadata['export_type'] = 'markdown'\n",
    "                    \n",
    "                    chunks.append(Document(\n",
    "                        page_content=split.page_content if hasattr(split, 'page_content') else split,\n",
    "                        metadata=metadata\n",
    "                    ))\n",
    "                \n",
    "                return chunks\n",
    "            else:\n",
    "                # Use simple text splitting\n",
    "                texts = self.text_splitter.split_text(markdown_content)\n",
    "                \n",
    "                chunks = []\n",
    "                for i, text in enumerate(texts):\n",
    "                    chunks.append(Document(\n",
    "                        page_content=text,\n",
    "                        metadata={\n",
    "                            'source': str(pdf_path.name),\n",
    "                            'chunk_index': i,\n",
    "                            'export_type': 'text_split',\n",
    "                            'num_pages': len(doc.pages) if hasattr(doc, 'pages') else 0\n",
    "                        }\n",
    "                    ))\n",
    "                \n",
    "                return chunks\n",
    "    \n",
    "    def _process_chunks_with_embeddings(\n",
    "        self, \n",
    "        chunks: List[Document], \n",
    "        pdf_path: Path\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Process chunks in batches to create embeddings and categorize\"\"\"\n",
    "        chunks_data = []\n",
    "        batch_texts = []\n",
    "        batch_metadata = []\n",
    "        \n",
    "        self.logger.info(f\"  -> Creating embeddings for {len(chunks)} chunks...\")\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            batch_texts.append(chunk.page_content)\n",
    "            batch_metadata.append(chunk.metadata)\n",
    "            \n",
    "            # Process batch when full or at end\n",
    "            if len(batch_texts) >= self.config.batch_size or i == len(chunks) - 1:\n",
    "                # Create embeddings for batch\n",
    "                embeddings = self.embeddings_model.embed_documents(batch_texts)\n",
    "                \n",
    "                # Process each chunk in batch\n",
    "                for text, embedding, metadata in zip(batch_texts, embeddings, batch_metadata):\n",
    "                    # Categorize using embedding similarity\n",
    "                    category_1, similarity_score = self._categorize_by_embedding_similarity(\n",
    "                        embedding, self.category_embeddings\n",
    "                    )\n",
    "                    \n",
    "                    # Extract IFRS standard from metadata\n",
    "                    category_2 = self._extract_standard_from_metadata(metadata, text)\n",
    "                    \n",
    "                    # Check for special content\n",
    "                    has_table = self._contains_table(metadata, text)\n",
    "                    has_figure = self._contains_figure(metadata, text)\n",
    "                    \n",
    "                    if has_table:\n",
    "                        self.stats.tables_found += 1\n",
    "                    if has_figure:\n",
    "                        self.stats.figures_found += 1\n",
    "                    \n",
    "                    # Create chunk data\n",
    "                    chunk_data = {\n",
    "                        'content': text,\n",
    "                        'embedding': embedding,\n",
    "                        'category_1': category_1,\n",
    "                        'category_1_similarity': similarity_score,\n",
    "                        'category_2': category_2,\n",
    "                        'source_file': str(pdf_path.name),\n",
    "                        'page_no': metadata.get('page_no', -1),\n",
    "                        'section': self._extract_section(metadata),\n",
    "                        'has_table': has_table,\n",
    "                        'has_figure': has_figure,\n",
    "                        'chunk_index': i,\n",
    "                        'chunk_length': len(text),\n",
    "                        'metadata': metadata\n",
    "                    }\n",
    "                    \n",
    "                    chunks_data.append(chunk_data)\n",
    "                    self.stats.total_chunks += 1\n",
    "                \n",
    "                # Clear batch\n",
    "                batch_texts = []\n",
    "                batch_metadata = []\n",
    "                \n",
    "                # Log progress\n",
    "                if (i + 1) % 100 == 0:\n",
    "                    self.logger.info(f\"    Processed {i + 1}/{len(chunks)} chunks\")\n",
    "        \n",
    "        return chunks_data\n",
    "    \n",
    "    def _categorize_by_embedding_similarity(\n",
    "        self, \n",
    "        chunk_embedding: np.ndarray, \n",
    "        category_embeddings: Dict[str, np.ndarray]\n",
    "    ) -> Tuple[str, float]:\n",
    "        \"\"\"\n",
    "        Categorize chunk based on embedding similarity to category descriptions.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (category, similarity_score)\n",
    "        \"\"\"\n",
    "        similarities = {}\n",
    "        chunk_embedding = np.array(chunk_embedding)\n",
    "        \n",
    "        for category, cat_embedding in category_embeddings.items():\n",
    "            similarity = self._compute_cosine_similarity(chunk_embedding, cat_embedding)\n",
    "            similarities[category] = similarity\n",
    "        \n",
    "        # Get category with highest similarity\n",
    "        best_category = max(similarities, key=similarities.get)\n",
    "        best_score = similarities[best_category]\n",
    "        \n",
    "        # Log if similarity is low\n",
    "        if best_score < 0.5:\n",
    "            self.logger.debug(\n",
    "                f\"    Low similarity score ({best_score:.3f}) for category {best_category}\"\n",
    "            )\n",
    "        \n",
    "        return best_category, best_score\n",
    "    \n",
    "    def _extract_standard_from_metadata(self, metadata: Dict, text: str) -> str:\n",
    "        \"\"\"Extract IFRS standard from metadata or text\"\"\"\n",
    "        # First check hierarchical metadata from MarkdownHeaderTextSplitter\n",
    "        if 'Standard' in metadata:\n",
    "            standard_text = metadata['Standard']\n",
    "            for standard in self.categories_2:\n",
    "                if standard.upper() in standard_text.upper():\n",
    "                    return standard\n",
    "        \n",
    "        # Check all header levels\n",
    "        for header_level in ['Section', 'Subsection', 'Paragraph']:\n",
    "            if header_level in metadata:\n",
    "                header_text = metadata[header_level]\n",
    "                for standard in self.categories_2:\n",
    "                    if standard.upper() in header_text.upper():\n",
    "                        return standard\n",
    "        \n",
    "        # Check metadata headings\n",
    "        headings = metadata.get('headings', [])\n",
    "        for heading in headings:\n",
    "            for standard in self.categories_2:\n",
    "                if standard.upper() in str(heading).upper():\n",
    "                    return standard\n",
    "        \n",
    "        # Check source filename\n",
    "        source = metadata.get('source', '')\n",
    "        for standard in self.categories_2:\n",
    "            if standard.replace(' ', '').upper() in source.upper():\n",
    "                return standard\n",
    "        \n",
    "        # Check text content\n",
    "        text_upper = text[:500].upper()  # Check first 500 chars\n",
    "        for standard in self.categories_2:\n",
    "            if standard.upper() in text_upper:\n",
    "                return standard\n",
    "        \n",
    "        return 'General'\n",
    "    \n",
    "    def _extract_section(self, metadata: Dict) -> str:\n",
    "        \"\"\"Extract section hierarchy from metadata\"\"\"\n",
    "        # First check for headers from MarkdownHeaderTextSplitter\n",
    "        if 'Standard' in metadata:\n",
    "            sections = []\n",
    "            for level in ['Standard', 'Section', 'Subsection', 'Paragraph']:\n",
    "                if level in metadata:\n",
    "                    sections.append(metadata[level])\n",
    "            if sections:\n",
    "                return ' > '.join(sections)\n",
    "        \n",
    "        # Fallback to other metadata\n",
    "        headings = metadata.get('headings', [])\n",
    "        if headings:\n",
    "            return ' > '.join(str(h) for h in headings)\n",
    "        return metadata.get('section', 'Root')\n",
    "    \n",
    "    def _contains_table(self, metadata: Dict, text: str) -> bool:\n",
    "        \"\"\"Check if chunk contains table\"\"\"\n",
    "        # Check metadata\n",
    "        doc_items = metadata.get('dl_meta', {}).get('doc_items', [])\n",
    "        if any(item.get('label') == 'table' for item in doc_items):\n",
    "            return True\n",
    "        \n",
    "        # Fallback: check text patterns\n",
    "        table_indicators = ['|', '┌', '├', '│', 'Table ', 'TABLE ']\n",
    "        return any(indicator in text for indicator in table_indicators)\n",
    "    \n",
    "    def _contains_figure(self, metadata: Dict, text: str) -> bool:\n",
    "        \"\"\"Check if chunk contains figure/picture\"\"\"\n",
    "        # Check metadata\n",
    "        doc_items = metadata.get('dl_meta', {}).get('doc_items', [])\n",
    "        if any(item.get('label') in ['picture', 'figure'] for item in doc_items):\n",
    "            return True\n",
    "        \n",
    "        # Fallback: check text patterns\n",
    "        figure_indicators = ['Figure ', 'FIGURE ', 'Exhibit ', 'Chart ', 'Diagram ']\n",
    "        return any(indicator in text for indicator in figure_indicators)\n",
    "    \n",
    "    def _log_final_statistics(self, df: pd.DataFrame):\n",
    "        \"\"\"Log comprehensive final statistics\"\"\"\n",
    "        stats = self.stats.get_summary()\n",
    "        \n",
    "        self.logger.info(\"\\n\" + \"=\"*80)\n",
    "        self.logger.info(\"PROCESSING COMPLETE - FINAL STATISTICS\")\n",
    "        self.logger.info(\"=\"*80)\n",
    "        \n",
    "        self.logger.info(f\"\\nDocument Processing:\")\n",
    "        self.logger.info(f\"  Total PDFs: {stats['total_pdfs']}\")\n",
    "        self.logger.info(f\"  Successfully processed: {stats['processed_pdfs']}\")\n",
    "        self.logger.info(f\"  Failed: {stats['failed_pdfs']}\")\n",
    "        self.logger.info(f\"  Success rate: {stats['success_rate']:.1f}%\")\n",
    "        \n",
    "        self.logger.info(f\"\\nContent Statistics:\")\n",
    "        self.logger.info(f\"  Total chunks created: {stats['total_chunks']}\")\n",
    "        self.logger.info(f\"  Total pages processed: {stats['total_pages']}\")\n",
    "        self.logger.info(f\"  Average chunks per document: {stats['avg_chunks_per_doc']:.1f}\")\n",
    "        self.logger.info(f\"  Tables found: {stats['tables_found']}\")\n",
    "        self.logger.info(f\"  Figures found: {stats['figures_found']}\")\n",
    "        \n",
    "        self.logger.info(f\"\\nPerformance:\")\n",
    "        self.logger.info(f\"  Total processing time: {stats['total_time_seconds']:.2f}s\")\n",
    "        self.logger.info(f\"  Average time per document: {stats['avg_time_per_doc']:.2f}s\")\n",
    "        \n",
    "        if len(df) > 0:\n",
    "            self.logger.info(f\"\\nCategorization Results:\")\n",
    "            self.logger.info(\"  Category 1 distribution:\")\n",
    "            for cat, count in df['category_1'].value_counts().items():\n",
    "                pct = count/len(df)*100\n",
    "                avg_sim = df[df['category_1']==cat]['category_1_similarity'].mean()\n",
    "                self.logger.info(f\"    {cat}: {count} ({pct:.1f}%) - avg similarity: {avg_sim:.3f}\")\n",
    "            \n",
    "            self.logger.info(\"\\n  Category 2 distribution (top 10):\")\n",
    "            for cat, count in df['category_2'].value_counts().head(10).items():\n",
    "                pct = count/len(df)*100\n",
    "                self.logger.info(f\"    {cat}: {count} ({pct:.1f}%)\")\n",
    "            \n",
    "            self.logger.info(f\"\\nContent Analysis:\")\n",
    "            self.logger.info(f\"  Average chunk length: {df['chunk_length'].mean():.0f} characters\")\n",
    "            self.logger.info(f\"  Chunks with tables: {df['has_table'].sum()} ({df['has_table'].sum()/len(df)*100:.1f}%)\")\n",
    "            self.logger.info(f\"  Chunks with figures: {df['has_figure'].sum()} ({df['has_figure'].sum()/len(df)*100:.1f}%)\")\n",
    "            \n",
    "            # Show sample hierarchical structure\n",
    "            self.logger.info(f\"\\nSample Document Structure:\")\n",
    "            sample_sections = df[df['section'] != 'Root']['section'].head(5)\n",
    "            for section in sample_sections:\n",
    "                self.logger.info(f\"  {section}\")\n",
    "        \n",
    "        self.logger.info(\"\\n\" + \"=\"*80)\n",
    "\n",
    "\n",
    "def test_imports():\n",
    "    \"\"\"Test function to diagnose import issues\"\"\"\n",
    "    print(\"\\n=== Testing Imports ===\")\n",
    "    \n",
    "    # Helper for safe printing\n",
    "    def safe_print(msg, success=True):\n",
    "        try:\n",
    "            print(msg)\n",
    "        except UnicodeEncodeError:\n",
    "            # Fallback for Windows\n",
    "            if success:\n",
    "                print(msg.replace('✓', '[OK]'))\n",
    "            else:\n",
    "                print(msg.replace('✗', '[FAIL]'))\n",
    "    \n",
    "    # Test Docling\n",
    "    try:\n",
    "        import docling\n",
    "        safe_print(\"[OK] docling is installed\")\n",
    "    except ImportError:\n",
    "        safe_print(\"[FAIL] docling is not installed - run: pip install docling\", False)\n",
    "    \n",
    "    # Test langchain_docling with correct import paths\n",
    "    try:\n",
    "        import langchain_docling\n",
    "        safe_print(\"[OK] langchain_docling is installed\")\n",
    "        \n",
    "        # Test DoclingLoader import\n",
    "        try:\n",
    "            from langchain_docling import DoclingLoader\n",
    "            safe_print(\"  [OK] DoclingLoader imported successfully\")\n",
    "        except ImportError as e:\n",
    "            safe_print(f\"  [FAIL] DoclingLoader import failed: {e}\", False)\n",
    "        \n",
    "        # Test ExportType import from correct location\n",
    "        try:\n",
    "            from langchain_docling.loader import ExportType\n",
    "            safe_print(\"  [OK] ExportType imported from langchain_docling.loader\")\n",
    "            safe_print(f\"    Available export types: {[e.name for e in ExportType]}\")\n",
    "        except ImportError as e:\n",
    "            safe_print(f\"  [FAIL] ExportType import failed: {e}\", False)\n",
    "            \n",
    "    except ImportError:\n",
    "        safe_print(\"[FAIL] langchain_docling is not installed - run: pip install langchain-docling\", False)\n",
    "    \n",
    "    # Test other dependencies\n",
    "    try:\n",
    "        from langchain_openai import OpenAIEmbeddings\n",
    "        safe_print(\"[OK] langchain_openai is installed\")\n",
    "    except ImportError:\n",
    "        safe_print(\"[FAIL] langchain_openai is not installed - run: pip install langchain-openai\", False)\n",
    "    \n",
    "    try:\n",
    "        from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "        safe_print(\"[OK] langchain_text_splitters is installed\")\n",
    "    except ImportError:\n",
    "        safe_print(\"[FAIL] langchain_text_splitters is not installed - run: pip install langchain-text-splitters\", False)\n",
    "    \n",
    "    print(\"======================\\n\")\n",
    "\n",
    "\n",
    "\"\"\"Example usage of the IFRS Document Processor\"\"\"\n",
    "\n",
    "# Test imports first\n",
    "test_imports()\n",
    "\n",
    "# Define categorization schemes\n",
    "categories_1 = [\n",
    "    'Recognition',\n",
    "    'Measurement', \n",
    "    'Disclosure',\n",
    "    'Presentation',\n",
    "    'Transition'\n",
    "]\n",
    "\n",
    "categories_2 = [\n",
    "    'IFRS 15', 'IFRS 16', 'IFRS 9', 'IFRS 13', 'IFRS 3',\n",
    "    'IAS 12', 'IAS 19', 'IAS 16', 'IAS 36', 'IAS 37',\n",
    "    'General'\n",
    "]\n",
    "\n",
    "# Configure processing\n",
    "config = ProcessingConfig(\n",
    "    enable_table_extraction=True,\n",
    "    enable_figure_descriptions=True,\n",
    "    embedding_model=\"text-embedding-3-large\",\n",
    "    embedding_dimensions=3072,\n",
    "    batch_size=50,\n",
    "    log_level=\"INFO\",\n",
    "    log_file=\"ifrs_processing.log\"\n",
    ")\n",
    "\n",
    "# Initialize processor\n",
    "processor = IFRSDocumentProcessor(\n",
    "    category_list_1=categories_1,\n",
    "    category_list_2=categories_2,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Process documents\n",
    "pdf_dir = Path(\"../data/knowledge_base/ifrs_test\")\n",
    "pdf_files = list(pdf_dir.glob(\"*.pdf\"))\n",
    "\n",
    "if not pdf_files:\n",
    "    logging.error(f\"No PDF files found in {pdf_dir}\")\n",
    "\n",
    "logging.info(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "\n",
    "# Process and create dataframe\n",
    "df = processor.process_documents(pdf_files)\n",
    "\n",
    "# Save to parquet with partitioning\n",
    "output_dir = Path(\"output\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_path = output_dir / \"ifrs_embeddings.parquet\"\n",
    "\n",
    "if len(df) > 0:\n",
    "    df.to_parquet(\n",
    "        output_path,\n",
    "        partition_cols=['category_2'],  # Partition by IFRS standard\n",
    "        compression='snappy',\n",
    "        index=False\n",
    "    )\n",
    "    \n",
    "    logging.info(f\"\\n[OK] Saved processed data to {output_path}\")\n",
    "    \n",
    "    # Save category statistics\n",
    "    category_stats = df.groupby(['category_1', 'category_2']).agg({\n",
    "        'content': 'count',\n",
    "        'category_1_similarity': 'mean',\n",
    "        'has_table': 'sum',\n",
    "        'has_figure': 'sum'\n",
    "    }).round(3)\n",
    "    category_stats.columns = ['chunk_count', 'avg_similarity', 'tables', 'figures']\n",
    "    category_stats.to_csv(output_dir / \"category_statistics.csv\")\n",
    "    \n",
    "    # Save processing statistics\n",
    "    stats_df = pd.DataFrame([processor.stats.get_summary()])\n",
    "    stats_df.to_csv(output_dir / \"processing_statistics.csv\", index=False)\n",
    "    \n",
    "    # Save a sample for inspection\n",
    "    sample_df = df.drop(columns=['embedding']).head(20)\n",
    "    sample_df.to_csv(output_dir / \"sample_chunks.csv\", index=False)\n",
    "    \n",
    "    logging.info(\"[OK] Saved statistics and sample files\")\n",
    "else:\n",
    "    logging.warning(\"No data to save - DataFrame is empty\")\n",
    "\n",
    "logging.info(\"\\n[COMPLETE] Processing pipeline completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e2b1e83-5144-42c7-957d-fd33aa3f4e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>embedding</th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_1_similarity</th>\n",
       "      <th>category_2</th>\n",
       "      <th>source_file</th>\n",
       "      <th>page_no</th>\n",
       "      <th>section</th>\n",
       "      <th>has_table</th>\n",
       "      <th>has_figure</th>\n",
       "      <th>chunk_index</th>\n",
       "      <th>chunk_length</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Luxembourg: Publications Office of the Europea...</td>\n",
       "      <td>[-0.005151908844709396, 0.03033466637134552, -...</td>\n",
       "      <td>Transition</td>\n",
       "      <td>0.199280</td>\n",
       "      <td>IFRS 9</td>\n",
       "      <td>IFRS9 monitoring report.pdf</td>\n",
       "      <td>-1</td>\n",
       "      <td>Root</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>49</td>\n",
       "      <td>59</td>\n",
       "      <td>{'source': 'IFRS9 monitoring report.pdf', 'chu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>## © European Banking Authority, 2021  \\nRepro...</td>\n",
       "      <td>[-0.026282820850610733, -0.012836170382797718,...</td>\n",
       "      <td>Disclosure</td>\n",
       "      <td>0.235516</td>\n",
       "      <td>IFRS 9</td>\n",
       "      <td>IFRS9 monitoring report.pdf</td>\n",
       "      <td>-1</td>\n",
       "      <td>Root</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>49</td>\n",
       "      <td>291</td>\n",
       "      <td>{'Section': '© European Banking Authority, 202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>## IFRS9 IMPLEMENTATION BY EU INSTITUTIONS  \\n...</td>\n",
       "      <td>[-0.02926705777645111, 0.018509436398744583, -...</td>\n",
       "      <td>Measurement</td>\n",
       "      <td>0.359255</td>\n",
       "      <td>IFRS 9</td>\n",
       "      <td>IFRS9 monitoring report.pdf</td>\n",
       "      <td>-1</td>\n",
       "      <td>Root</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>49</td>\n",
       "      <td>62</td>\n",
       "      <td>{'Section': 'IFRS9 IMPLEMENTATION BY EU INSTIT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>## Contents  \\n| List of figures              ...</td>\n",
       "      <td>[-0.005802014376968145, 0.00991261750459671, -...</td>\n",
       "      <td>Transition</td>\n",
       "      <td>0.350552</td>\n",
       "      <td>IFRS 9</td>\n",
       "      <td>IFRS9 monitoring report.pdf</td>\n",
       "      <td>-1</td>\n",
       "      <td>Root</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>49</td>\n",
       "      <td>12230</td>\n",
       "      <td>{'Section': 'Contents', 'source': 'IFRS9 monit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>## List of Figures  \\n| Figure 1.   | Represen...</td>\n",
       "      <td>[-0.01747412420809269, 0.02101113460958004, -0...</td>\n",
       "      <td>Transition</td>\n",
       "      <td>0.371055</td>\n",
       "      <td>IFRS 9</td>\n",
       "      <td>IFRS9 monitoring report.pdf</td>\n",
       "      <td>-1</td>\n",
       "      <td>Root</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>49</td>\n",
       "      <td>18705</td>\n",
       "      <td>{'Section': 'List of Figures', 'source': 'IFRS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Luxembourg: Publications Office of the Europea...   \n",
       "1  ## © European Banking Authority, 2021  \\nRepro...   \n",
       "2  ## IFRS9 IMPLEMENTATION BY EU INSTITUTIONS  \\n...   \n",
       "3  ## Contents  \\n| List of figures              ...   \n",
       "4  ## List of Figures  \\n| Figure 1.   | Represen...   \n",
       "\n",
       "                                           embedding   category_1  \\\n",
       "0  [-0.005151908844709396, 0.03033466637134552, -...   Transition   \n",
       "1  [-0.026282820850610733, -0.012836170382797718,...   Disclosure   \n",
       "2  [-0.02926705777645111, 0.018509436398744583, -...  Measurement   \n",
       "3  [-0.005802014376968145, 0.00991261750459671, -...   Transition   \n",
       "4  [-0.01747412420809269, 0.02101113460958004, -0...   Transition   \n",
       "\n",
       "   category_1_similarity category_2                  source_file  page_no  \\\n",
       "0               0.199280     IFRS 9  IFRS9 monitoring report.pdf       -1   \n",
       "1               0.235516     IFRS 9  IFRS9 monitoring report.pdf       -1   \n",
       "2               0.359255     IFRS 9  IFRS9 monitoring report.pdf       -1   \n",
       "3               0.350552     IFRS 9  IFRS9 monitoring report.pdf       -1   \n",
       "4               0.371055     IFRS 9  IFRS9 monitoring report.pdf       -1   \n",
       "\n",
       "  section  has_table  has_figure  chunk_index  chunk_length  \\\n",
       "0    Root      False       False           49            59   \n",
       "1    Root      False       False           49           291   \n",
       "2    Root      False       False           49            62   \n",
       "3    Root       True       False           49         12230   \n",
       "4    Root       True        True           49         18705   \n",
       "\n",
       "                                            metadata  \n",
       "0  {'source': 'IFRS9 monitoring report.pdf', 'chu...  \n",
       "1  {'Section': '© European Banking Authority, 202...  \n",
       "2  {'Section': 'IFRS9 IMPLEMENTATION BY EU INSTIT...  \n",
       "3  {'Section': 'Contents', 'source': 'IFRS9 monit...  \n",
       "4  {'Section': 'List of Figures', 'source': 'IFRS...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a4cf3a-ff12-4753-b1e8-894350ee3a69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
